## Extraction Workflow Overview

This folder keeps the entire Island Troll Tribes data-extraction experiment inside the main repo so it can be versioned with the site. All scripts still expect to be run from the repo root (`ittweb/`) because they read and write relative paths under `external/wurst` and `src/features`.

### Layout
- `docs/features/extraction/` – repository-level documentation area that now houses the archived progress/status reports under `archive/`.
- `data/` – the JSON and TypeScript artifacts generated by the experiments (`abilities.json`, `recipes.json`, `subclass_superclass_data.ts`, etc.); see `data/README.md` for producers and consumers.
- `scripts/current/` – the full toolbox of Python utilities used during the original extraction effort. Includes assets such as `LifeCraft_Font.ttf` referenced by loading-screen scripts.
- `scripts/archive/` – earlier prototypes from `scripts_1/`; handy for diffing or salvaging logic when consolidating.

## Data Flow
1. **Source** – Warcraft III map logic lives under `external/island_troll_tribes/wurst`. Scripts parse `.wurst`, `.slk`, and auxiliary files directly.
2. **Extraction** – `extract_*.py` scripts produce normalized JSON (saved into `src/features/infrastructure/extraction/data/` after running).
3. **Generation** – `generate_*.py` scripts read those JSON files and emit strongly typed `.ts` files for the web app (abilities, buildings, classes, external items).
4. **Integration** – scripts such as `integrate_all_data.py` enrich generated files with descriptions and stats, keeping manual overrides (`items.raw-materials.ts`, etc.) intact.
5. **Reset/Rebuild** – `reset_all_data.py` cleans previously generated artifacts while preserving curated base files; `recreate_all_data.py` re-runs the complete pipeline.

## Script Inventory (current/)

| Category | Key Scripts | Notes |
| --- | --- | --- |
| Extraction | `extract_ability_descriptions.py`, `extract_abilities_from_wurst.py`, `extract_recipes.py`, `extract_item_stats.py`, `extract_buildings.py`, `extract_units.py` | Pull raw data from Wurst sources into JSON. Most rely on regex-based parsing. |
| Generation | `generate_abilities_from_json.py`, `generate_buildings_from_json.py`, `generate_classes_from_json.py`, `generate_external_items_from_recipes.py`, `generate_items_ts.py`, `generate_base_items_from_recipes.py` | Convert JSON to TypeScript modules consumed by the site. |
| Integration | `integrate_all_data.py`, `validate_outputs.py` | Enrich generated files with descriptions/stats and assert JSON sanity before publishing. |
| Workflow | `manage_extraction.py`, `reset_all_data.py`, `recreate_all_data.py` | Core orchestration helpers that chain extraction + generation steps. |

Prototype scripts under `scripts/archive/` are earlier iterations of the same logic. When consolidating into a single master script, mine these for niche behaviors that never made it into the current versions.

## How to Run the Existing Pipeline

All commands assume the working directory is the repo root:

```powershell
# Quick view of available steps
python src/features/infrastructure/extraction/scripts/current/manage_extraction.py list

# Full pipeline (reset + extract + generate + integrate)
python src/features/infrastructure/extraction/scripts/current/manage_extraction.py pipeline

# Via package script (runs pipeline)
npm run extract:data

# Only rerun extraction phase
python src/features/infrastructure/extraction/scripts/current/manage_extraction.py extract

# Skip reset but run everything else
python src/features/infrastructure/extraction/scripts/current/manage_extraction.py pipeline --skip-reset

# Only run validation checks
python src/features/infrastructure/extraction/scripts/current/manage_extraction.py validate
```

### Module Tests
```powershell
python -m unittest discover src/features/infrastructure/extraction/scripts/current/tests
```

To execute individual phases:

```powershell
# Extraction
python src/features/infrastructure/extraction/scripts/current/extract_ability_descriptions.py
python src/features/infrastructure/extraction/scripts/current/extract_abilities_from_wurst.py
python src/features/infrastructure/extraction/scripts/current/extract_item_stats.py
python src/features/infrastructure/extraction/scripts/current/extract_buildings.py
python src/features/infrastructure/extraction/scripts/current/extract_units.py
python src/features/infrastructure/extraction/scripts/current/extract_recipes.py

# Generation
python src/features/infrastructure/extraction/scripts/current/generate_abilities_from_json.py
python src/features/infrastructure/extraction/scripts/current/generate_buildings_from_json.py
python src/features/infrastructure/extraction/scripts/current/generate_classes_from_json.py
python src/features/infrastructure/extraction/scripts/current/generate_external_items_from_recipes.py

# Integration / cleanup
python src/features/infrastructure/extraction/scripts/current/integrate_all_data.py
```

## Next Steps Toward a Formalized Process
- **Script consolidation** – evaluate which helpers are still necessary, then design a single orchestrator (or CLI) that sequences extraction, generation, and integration with explicit flags.
- **Dependency pinning** – install requirements with `python -m pip install -r src/features/infrastructure/extraction/scripts/requirements.txt`; update pins via `pip-compile` if packages change.
- **Automation hooks** – decide where this pipeline should run (local CLI, CI job, scheduled task) and document expected inputs/outputs for downstream site builds.
- **Validation** – extend `validate_outputs.py` (and the unit tests under `scripts/current/tests/`) so regressions are caught immediately after extraction/generation.

With the assets now inside `src/features/infrastructure/extraction/`, we can iterate on the process without depending on the old `external` sandbox.

